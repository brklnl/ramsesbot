{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow2Vis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow2vis(flownp, maxF=500.0, n=8, mask=None, hueMax=179, angShift=0.0): \n",
    "\n",
    "    ang, mag, _ = _calculate_angle_distance_from_du_dv( flownp[:, :, 0], flownp[:, :, 1], flagDegree=False )\n",
    "\n",
    "    # Use Hue, Saturation, Value colour model \n",
    "    hsv = np.zeros( ( ang.shape[0], ang.shape[1], 3 ) , dtype=np.float32)\n",
    "\n",
    "    am = ang < 0\n",
    "    ang[am] = ang[am] + np.pi * 2\n",
    "\n",
    "    hsv[ :, :, 0 ] = np.remainder( ( ang + angShift ) / (2*np.pi), 1 )\n",
    "    hsv[ :, :, 1 ] = mag / maxF * n\n",
    "    hsv[ :, :, 2 ] = (n - hsv[:, :, 1])/n\n",
    "\n",
    "    hsv[:, :, 0] = np.clip( hsv[:, :, 0], 0, 1 ) * hueMax\n",
    "    hsv[:, :, 1:3] = np.clip( hsv[:, :, 1:3], 0, 1 ) * 255\n",
    "    hsv = hsv.astype(np.uint8)\n",
    "\n",
    "    rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "\n",
    "    if ( mask is not None ):\n",
    "        mask = mask > 0\n",
    "        rgb[mask] = np.array([0, 0 ,0], dtype=np.uint8)\n",
    "\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters `maxF`, `n`, `hueMax`, and `angShift` in the `flow2vis` function are used to control the appearance of the optical flow visualization. The \"best\" values for these parameters can depend on your specific use case and personal preference. Here are some general guidelines:\n",
    "\n",
    "1. `maxF`: This parameter controls the maximum flow that will be mapped to the highest color intensity. If your optical flow values are generally small, you might want to decrease `maxF` so that small flows are still visible in the visualization. If your optical flow values are large, you might want to increase `maxF` so that the visualization isn't saturated with high-intensity colors.\n",
    "\n",
    "2. `n`: This parameter is a scaling factor used in the computation of the Saturation and Value components of the HSV color space. If your visualization appears too dark or too light, you might want to adjust `n`.\n",
    "\n",
    "3. `hueMax`: This parameter controls the maximum value for the Hue component in the HSV color space. In OpenCV, the maximum Hue value is typically 179. You generally shouldn't need to change this unless you want to use a different color space.\n",
    "\n",
    "4. `angShift`: This parameter is a shift applied to the flow angles before computing the Hue component. It can be used to rotate the color wheel used in the visualization. If you want to emphasize certain flow directions over others, you might want to adjust `angShift`.\n",
    "\n",
    "To find the best values for these parameters, you could try different combinations and see which one produces a visualization that best meets your needs. You could also consider using a method like grid search or random search to systematically explore the parameter space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub Copilot: You can use the `cp` command in the terminal to copy files. Here's a command that will copy the first 200 images from one directory to another:\n",
    "\n",
    "```bash\n",
    "for file in $(ls -v /path/to/source/directory/*.jpg | head -200); do cp \"$file\" /path/to/destination/directory/; done\n",
    "```\n",
    "```bash\n",
    "for file in $(ls -v /path/to/source/directory/*.jpg | sed -n '200,250p'); do cp \"$file\" /path/to/destination/directory/; done\n",
    "```\n",
    "This command does the following:\n",
    "\n",
    "1. `ls /path/to/source/directory/*.jpg` lists all the .jpg files in the source directory.\n",
    "2. `head -200` takes the first 200 files from this list.\n",
    "3. `for file in $(...); do ... done` loops over these 200 files.\n",
    "4. `cp \"$file\" /path/to/destination/directory/` copies each file to the destination directory.\n",
    "\n",
    "Replace `/path/to/source/directory/` with the path to your source directory and `/path/to/destination/directory/` with the path to your destination directory. If your images are not .jpg, replace .jpg with the correct file extension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OF handling last image in the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub Copilot: The code you've posted is using the modulo operator (`%`) to calculate `next_flow_filename`. This operation returns the remainder of the division of `idx + 1` by `total_images`. \n",
    "\n",
    "When `idx + 1` equals `total_images` (i.e., when `idx` is the index of the last image), `idx + 1` divided by `total_images` has a remainder of 0. Therefore, `(idx + 1) % total_images` equals 0, and `next_flow_filename` is set to the filename of the first image in your dataset.\n",
    "\n",
    "This is why you didn't get an error when processing the last image in your dataset: instead of trying to load an image that doesn't exist, your code wrapped around to the start of the dataset and loaded the first image as the \"next\" image.\n",
    "\n",
    "If you want to keep this behavior when adding the validation process, you can modify your updated code like this:\n",
    "\n",
    "```python\n",
    "# Apply the offset to the index for current and next images\n",
    "current_idx = idx \n",
    "next_idx = (idx + 1 ) % total_images\n",
    "\n",
    "current_flow_filename = f\"{current_idx:06d}_left.png\"\n",
    "\n",
    "current_image_path = os.path.join(self.segmentation_images_path, current_flow_filename)\n",
    "current_image = Image.open(current_image_path).convert('RGB')\n",
    "\n",
    "next_flow_filename = f\"{next_idx:06d}_left.png\"\n",
    "next_image_path = os.path.join(self.segmentation_images_path, next_flow_filename)\n",
    "next_image = Image.open(next_image_path).convert('RGB')\n",
    "```\n",
    "\n",
    "This way, `next_idx` will always be within range, and `next_image` will be set to the first image in your dataset when processing the last image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score flow and segmentation from SegFlow Caffe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub Copilot: This is a part of a Caffe model definition file. It defines several layers of a convolutional neural network (CNN) that are used for optical flow estimation. The layers include convolutional layers, deconvolutional layers, crop layers, concatenation layers, and an element-wise sum layer.\n",
    "\n",
    "Here's a rough equivalent in PyTorch. Note that PyTorch doesn't have a direct equivalent for the \"Crop\" layer in Caffe, so we'll use a function to achieve the same effect. Also, the \"Silence\" layer in Caffe is used to stop backpropagation, which is not needed in PyTorch as it handles backpropagation automatically.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def crop_like(input, target):\n",
    "    if input.size(2) != target.size(2) or input.size(3) != target.size(3):\n",
    "        return input[:, :, :target.size(2), :target.size(3)]\n",
    "    else:\n",
    "        return input\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 2, kernel_size=1)\n",
    "        self.deconv1 = nn.ConvTranspose2d(2, 2, kernel_size=4, stride=2, bias=False)\n",
    "        self.conv2 = nn.Conv2d(in_channels, 2, kernel_size=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(2, 2, kernel_size=4, stride=2, bias=False)\n",
    "        self.conv3 = nn.Conv2d(in_channels, 2, kernel_size=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(2, 2, kernel_size=16, stride=8, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # score_res5_con_flow\n",
    "        score_res5_flow = self.conv1(x)\n",
    "        # upscore2_flow\n",
    "        upscore2_flow = self.deconv1(score_res5_flow)\n",
    "        # score_res4_con_flow\n",
    "        score_res4_flow = self.conv2(x)\n",
    "        # upscore2c_flow\n",
    "        upscore2c_flow = crop_like(upscore2_flow, score_res4_flow)\n",
    "        # fuse_res4_flow\n",
    "        fuse_res4_flow = upscore2c_flow + score_res4_flow\n",
    "        # upscore_res4_flow\n",
    "        upscore_res4_flow = self.deconv2(fuse_res4_flow)\n",
    "        # score_res3_con_flow\n",
    "        score_res3_flow = self.conv3(x)\n",
    "        # upscore_res4c_flow\n",
    "        upscore_res4c_flow = crop_like(upscore_res4_flow, score_res3_flow)\n",
    "        # fuse_res3_flow\n",
    "        fuse_res3_flow = upscore_res4c_flow + score_res3_flow\n",
    "        # upscore8_flow\n",
    "        upscore8_flow = self.deconv3(fuse_res3_flow)\n",
    "        # score_flow\n",
    "        score_flow = crop_like(upscore8_flow, x)\n",
    "        return score_flow\n",
    "```\n",
    "\n",
    "Please note that you need to replace `in_channels` with the actual number of input channels for each convolutional layer. Also, this is a simplified version of the model, and you might need to adjust it according to your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#################################### score_flow #################################\n",
    "layer {\n",
    "  name: \"score_res5_con_flow\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"concat_res5\"\n",
    "  top: \"score_res5_flow\"\n",
    "  param {\n",
    "    lr_mult: 1\n",
    "    decay_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    lr_mult: 2\n",
    "    decay_mult: 0\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 2\n",
    "    pad: 0\n",
    "    kernel_size: 1\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"upscore2_flow\"\n",
    "  type: \"Deconvolution\"\n",
    "  bottom: \"score_res5_flow\"\n",
    "  top: \"upscore2_flow\"\n",
    "  param {\n",
    "    lr_mult: 0\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 2\n",
    "    bias_term: false\n",
    "    kernel_size: 4\n",
    "    stride: 2 \n",
    "  }\n",
    "}\n",
    "\n",
    "#### score_res4 ####\n",
    "\n",
    "layer {\n",
    "  name: \"concat3c_flow\"\n",
    "  type: \"Crop\"\n",
    "  bottom: \"pool_concat3\"\n",
    "  bottom: \"scale_res4\"\n",
    "  top: \"concat3c_flow\"\n",
    "  crop_param {\n",
    "    axis: 2\n",
    "    offset: 0\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"Concat_res4_flow\"\n",
    "  type: \"Concat\"\n",
    "  bottom: \"scale_res4\"\n",
    "  bottom: \"concat3c_flow\"\n",
    "  top: \"concat_res4_flow\"\n",
    "}\n",
    "\n",
    "\n",
    "layer {\n",
    "  name: \"score_res4_con_flow\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"concat_res4_flow\"\n",
    "  top: \"score_res4_flow\"\n",
    "  param {\n",
    "    lr_mult: 1\n",
    "    decay_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    lr_mult: 2\n",
    "    decay_mult: 0\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 2\n",
    "    pad: 0\n",
    "    kernel_size: 1\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"score_res4c_flow\"\n",
    "  type: \"Crop\"\n",
    "  bottom: \"upscore2_flow\"\n",
    "  bottom: \"score_res4_flow\"\n",
    "  top: \"upscore2c_flow\"\n",
    "  crop_param {\n",
    "    axis: 2\n",
    "    offset: 2\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"fuse_res4_flow\"\n",
    "  type: \"Eltwise\"\n",
    "  bottom: \"upscore2c_flow\"\n",
    "  bottom: \"score_res4_flow\"\n",
    "  top: \"fuse_res4_flow\"\n",
    "  eltwise_param {\n",
    "    operation: SUM\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"upscore_res4_flow\"\n",
    "  type: \"Deconvolution\"\n",
    "  bottom: \"fuse_res4_flow\"\n",
    "  top: \"upscore_res4_flow\"\n",
    "  param {\n",
    "    lr_mult: 0\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 2\n",
    "    bias_term: false\n",
    "    kernel_size: 4\n",
    "    stride: 2\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "#### score_res3 ####\n",
    "layer {\n",
    "  name: \"concat2c_flow\"\n",
    "  type: \"Crop\"\n",
    "  bottom: \"pool_concat2\"\n",
    "  bottom: \"scale_res3\"\n",
    "  top: \"concat2c_flow\"\n",
    "  crop_param {\n",
    "    axis: 2\n",
    "    offset: 0\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"Concat_res3_flow\"\n",
    "  type: \"Concat\"\n",
    "  bottom: \"scale_res3\"\n",
    "  bottom: \"concat2c_flow\"\n",
    "  top: \"concat_res3_flow\"\n",
    "}\n",
    "\n",
    "\n",
    "layer {\n",
    "  name: \"score_res3_con_flow\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"concat_res3_flow\"\n",
    "  top: \"score_res3_flow\"\n",
    "  param {\n",
    "    lr_mult: 1\n",
    "    decay_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    lr_mult: 2\n",
    "    decay_mult: 0\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 2\n",
    "    pad: 0\n",
    "    kernel_size: 1\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"score_res3c_flow\"\n",
    "  type: \"Crop\"\n",
    "  bottom: \"upscore_res4_flow\"\n",
    "  bottom: \"score_res3_flow\"\n",
    "  top: \"upscore_res4c_flow\"\n",
    "  crop_param {\n",
    "    axis: 2\n",
    "    offset: 2\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"fuse_res3_flow\"\n",
    "  type: \"Eltwise\"\n",
    "  bottom: \"upscore_res4c_flow\"\n",
    "  bottom: \"score_res3_flow\"\n",
    "  top: \"fuse_res3_flow\"\n",
    "  eltwise_param {\n",
    "    operation: SUM\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"upscore8_flow\"\n",
    "  type: \"Deconvolution\"\n",
    "  bottom: \"fuse_res3_flow\"\n",
    "  top: \"upscore8_flow\"\n",
    "  param {\n",
    "    lr_mult: 0\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 2\n",
    "    bias_term: false\n",
    "    kernel_size: 16\n",
    "    stride: 8\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"score_flow\"\n",
    "  type: \"Crop\"\n",
    "  bottom: \"upscore8_flow\"\n",
    "  bottom: \"data\"\n",
    "  top: \"score_flow\"\n",
    "  crop_param {\n",
    "    axis: 2\n",
    "    offset: 8\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "layer {\n",
    "  name: \"silence_layer_flow\" \n",
    "  type: \"Silence\" \n",
    "  bottom: \"score_flow\" \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub Copilot: This is a part of a Caffe model definition file. It defines several layers of a convolutional neural network (CNN) that are used for segmentation. The layers include convolutional layers, deconvolutional layers, crop layers, concatenation layers, and an element-wise sum layer.\n",
    "\n",
    "Here's a rough equivalent in PyTorch. Note that PyTorch doesn't have a direct equivalent for the \"Crop\" layer in Caffe, so we'll use a function to achieve the same effect. Also, the \"Silence\" layer in Caffe is used to stop backpropagation, which is not needed in PyTorch as it handles backpropagation automatically.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def crop_like(input, target):\n",
    "    if input.size(2) != target.size(2) or input.size(3) != target.size(3):\n",
    "        return input[:, :, :target.size(2), :target.size(3)]\n",
    "    else:\n",
    "        return input\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 2, kernel_size=1)\n",
    "        self.deconv1 = nn.ConvTranspose2d(2, 2, kernel_size=4, stride=2, bias=False)\n",
    "        self.conv2 = nn.Conv2d(in_channels, 2, kernel_size=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(2, 2, kernel_size=4, stride=2, bias=False)\n",
    "        self.conv3 = nn.Conv2d(in_channels, 2, kernel_size=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(2, 2, kernel_size=16, stride=8, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # score_res5_con\n",
    "        score_res5 = self.conv1(x)\n",
    "        # upscore2\n",
    "        upscore2 = self.deconv1(score_res5)\n",
    "        # score_res4_con\n",
    "        score_res4 = self.conv2(x)\n",
    "        # upscore2c\n",
    "        upscore2c = crop_like(upscore2, score_res4)\n",
    "        # fuse_res4\n",
    "        fuse_res4 = upscore2c + score_res4\n",
    "        # upscore_res4\n",
    "        upscore_res4 = self.deconv2(fuse_res4)\n",
    "        # score_res3_con\n",
    "        score_res3 = self.conv3(x)\n",
    "        # upscore_res4c\n",
    "        upscore_res4c = crop_like(upscore_res4, score_res3)\n",
    "        # fuse_res3\n",
    "        fuse_res3 = upscore_res4c + score_res3\n",
    "        # upscore8\n",
    "        upscore8 = self.deconv3(fuse_res3)\n",
    "        # score\n",
    "        score = crop_like(upscore8, x)\n",
    "        return score\n",
    "```\n",
    "\n",
    "Please note that you need to replace `in_channels` with the actual number of input channels for each convolutional layer. Also, this is a simplified version of the model, and you might need to adjust it according to your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#################################### score_segmentation #################################\n",
    "#### score_res5 ####\n",
    "layer {\n",
    "  bottom: \"concat4\"\n",
    "  top: \"pool_concat4\"\n",
    "  name: \"pool_concat4\"\n",
    "  type: \"Pooling\"\n",
    "  pooling_param {\n",
    "    kernel_size: 3\n",
    "    stride: 2\n",
    "    pool: MAX\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"concat4c\"\n",
    "  type: \"Crop\"\n",
    "  bottom: \"pool_concat4\"\n",
    "  bottom: \"res5c\"\n",
    "  top: \"concat4c\"\n",
    "  crop_param {\n",
    "    axis: 2\n",
    "    offset: 1\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"Concat_res5\"\n",
    "  type: \"Concat\"\n",
    "  bottom: \"res5c\"\n",
    "  bottom: \"concat4c\"\n",
    "  top: \"concat_res5\"\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"score_res5_con\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"concat_res5\"\n",
    "  top: \"score_res5\"\n",
    "  convolution_param {\n",
    "    num_output: 2\n",
    "    pad: 0\n",
    "    kernel_size: 1\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"upscore2\"\n",
    "  type: \"Deconvolution\"\n",
    "  bottom: \"score_res5\"\n",
    "  top: \"upscore2\"\n",
    "  param {\n",
    "    lr_mult: 0\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 2\n",
    "    bias_term: false\n",
    "    kernel_size: 4\n",
    "    stride: 2 \n",
    "  }\n",
    "}\n",
    "\n",
    "#### score_res4 ####\n",
    "layer {\n",
    "  name: \"scale_res4\"\n",
    "  type: \"Scale\"\n",
    "  bottom: \"res4b22\"\n",
    "  top: \"scale_res4\"\n",
    "  param {\n",
    "    lr_mult: 0\n",
    "  }\n",
    "  scale_param {\n",
    "    filler {\n",
    "      type: \"constant\"\n",
    "      value: 0.01\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  bottom: \"concat3\"\n",
    "  top: \"pool_concat3\"\n",
    "  name: \"pool_concat3\"\n",
    "  type: \"Pooling\"\n",
    "  pooling_param {\n",
    "    kernel_size: 3\n",
    "    stride: 2\n",
    "    pool: MAX\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"concat3c\"\n",
    "  type: \"Crop\"\n",
    "  bottom: \"pool_concat3\"\n",
    "  bottom: \"scale_res4\"\n",
    "  top: \"concat3c\"\n",
    "  crop_param {\n",
    "    axis: 2\n",
    "    offset: 2\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"Concat_res4\"\n",
    "  type: \"Concat\"\n",
    "  bottom: \"scale_res4\"\n",
    "  bottom: \"concat3c\"\n",
    "  top: \"concat_res4\"\n",
    "}\n",
    "\n",
    "\n",
    "layer {\n",
    "  name: \"score_res4_con\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"concat_res4\"\n",
    "  top: \"score_res4\"\n",
    "  param {\n",
    "    lr_mult: 1\n",
    "    decay_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    lr_mult: 2\n",
    "    decay_mult: 0\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 2\n",
    "    pad: 0\n",
    "    kernel_size: 1\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"score_res4c\"\n",
    "  type: \"Crop\"\n",
    "  bottom: \"upscore2\"\n",
    "  bottom: \"score_res4\"\n",
    "  top: \"upscore2c\"\n",
    "  crop_param {\n",
    "    axis: 2\n",
    "    offset: 2\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"fuse_res4\"\n",
    "  type: \"Eltwise\"\n",
    "  bottom: \"upscore2c\"\n",
    "  bottom: \"score_res4\"\n",
    "  top: \"fuse_res4\"\n",
    "  eltwise_param {\n",
    "    operation: SUM\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"upscore_res4\"\n",
    "  type: \"Deconvolution\"\n",
    "  bottom: \"fuse_res4\"\n",
    "  top: \"upscore_res4\"\n",
    "  param {\n",
    "    lr_mult: 0\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 2\n",
    "    bias_term: false\n",
    "    kernel_size: 4\n",
    "    stride: 2\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "#### score_res3 ####\n",
    "layer {\n",
    "  name: \"scale_res3\"\n",
    "  type: \"Scale\"\n",
    "  bottom: \"res3b3\"\n",
    "  top: \"scale_res3\"\n",
    "  param {\n",
    "    lr_mult: 0\n",
    "  }\n",
    "  scale_param {\n",
    "    filler {\n",
    "      type: \"constant\"\n",
    "      value: 0.0001\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "layer {\n",
    "  bottom: \"concat2\"\n",
    "  top: \"pool_concat2\"\n",
    "  name: \"pool_concat2\"\n",
    "  type: \"Pooling\"\n",
    "  pooling_param {\n",
    "    kernel_size: 3\n",
    "    stride: 2\n",
    "    pool: MAX\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"concat2c\"\n",
    "  type: \"Crop\"\n",
    "  bottom: \"pool_concat2\"\n",
    "  bottom: \"scale_res3\" \n",
    "  top: \"concat2c\"\n",
    "  crop_param {\n",
    "    axis: 2\n",
    "    offset: 4\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"Concat_res3\"\n",
    "  type: \"Concat\"\n",
    "  bottom: \"scale_res3\"\n",
    "  bottom: \"concat2c\"\n",
    "  top: \"concat_res3\"\n",
    "}\n",
    "\n",
    "\n",
    "layer {\n",
    "  name: \"score_res3_con\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"concat_res3\"\n",
    "  top: \"score_res3\"\n",
    "  param {\n",
    "    lr_mult: 1\n",
    "    decay_mult: 1\n",
    "  }\n",
    "  param {\n",
    "    lr_mult: 2\n",
    "    decay_mult: 0\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 2\n",
    "    pad: 0\n",
    "    kernel_size: 1\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"score_res3c\"\n",
    "  type: \"Crop\"\n",
    "  bottom: \"upscore_res4\"\n",
    "  bottom: \"score_res3\"\n",
    "  top: \"upscore_res4c\"\n",
    "  crop_param {\n",
    "    axis: 2\n",
    "    offset: 2\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"fuse_res3\"\n",
    "  type: \"Eltwise\"\n",
    "  bottom: \"upscore_res4c\"\n",
    "  bottom: \"score_res3\"\n",
    "  top: \"fuse_res3\"\n",
    "  eltwise_param {\n",
    "    operation: SUM\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"upscore8\"\n",
    "  type: \"Deconvolution\"\n",
    "  bottom: \"fuse_res3\"\n",
    "  top: \"upscore8\"\n",
    "  param {\n",
    "    lr_mult: 0\n",
    "  }\n",
    "  convolution_param {\n",
    "    num_output: 2\n",
    "    bias_term: false\n",
    "    kernel_size: 16\n",
    "    stride: 8\n",
    "  }\n",
    "}\n",
    "\n",
    "layer {\n",
    "  name: \"score\"\n",
    "  type: \"Crop\"\n",
    "  bottom: \"upscore8\"\n",
    "  bottom: \"data\"\n",
    "  top: \"score\"\n",
    "  crop_param {\n",
    "    axis: 2\n",
    "    offset: 8\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When optimizing the segmentation branch, we freeze the weights of the optical flow branch, and train the network on the DAVIS training set. We use SGD optimizer with batch size 1 for training, starting from learning rate 1e-8 and decreas-\n",
    "ing it by half for every 10000 iterations. For training the optical flow branch, similarly we fix the segmentation branch and only update the weights in the flow network using the target optical flow dataset . To balance the weights between two different losses, we use a smaller learning rate 1e-9 for the EPE loss , addressing the Î» in the combined loss. Note that, to decide when to switch the training process to another branch, we randomly split a validation set and stop training the current branch when the error on the validation set reaches a convergence. In addition, this validation set is also used to select the best model with respect to the iteration number.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement this iterative training process in PyTorch, you can follow these steps:\n",
    "\n",
    "1. **Initialize two optimizers**: One for the segmentation branch and one for the optical flow branch. You can use the SGD optimizer with different learning rates for each branch.\n",
    "\n",
    "2. **Freeze and unfreeze the appropriate layers**: Before training each branch, you need to freeze the weights of the other branch. You can do this by setting the `requires_grad` attribute of the parameters of the other branch to `False`.\n",
    "\n",
    "3. **Train each branch separately**: You can use a loop to train each branch for a certain number of iterations. After each iteration, you can compute the validation error and check if it has converged.\n",
    "\n",
    "Here is a simplified example of how you can implement this:\n",
    "\n",
    "```python\n",
    "# Initialize the optimizers\n",
    "optimizer_seg = torch.optim.SGD(segmentation_parameters, lr=1e-8)\n",
    "optimizer_flow = torch.optim.SGD(flow_parameters, lr=1e-9)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(max_epochs):\n",
    "    # Train segmentation branch\n",
    "    for param in flow_parameters:\n",
    "        param.requires_grad = False\n",
    "    for param in segmentation_parameters:\n",
    "        param.requires_grad = True\n",
    "\n",
    "    for i, data in enumerate(segmentation_train_loader):\n",
    "        # Training code for segmentation branch\n",
    "        # ...\n",
    "        if i % 10000 == 0:\n",
    "            for param_group in optimizer_seg.param_groups:\n",
    "                param_group['lr'] /= 2\n",
    "\n",
    "    # Check convergence on validation set\n",
    "    # If converged, switch to training flow branch\n",
    "\n",
    "    # Train flow branch\n",
    "    for param in segmentation_parameters:\n",
    "        param.requires_grad = False\n",
    "    for param in flow_parameters:\n",
    "        param.requires_grad = True\n",
    "\n",
    "    for i, data in enumerate(flow_train_loader):\n",
    "        # Training code for flow branch\n",
    "        # ...\n",
    "        if i % 10000 == 0:\n",
    "            for param_group in optimizer_flow.param_groups:\n",
    "                param_group['lr'] /= 2\n",
    "\n",
    "    # Check convergence on validation set\n",
    "    # If converged, switch to training segmentation branch\n",
    "```\n",
    "\n",
    "This is a simplified example and you'll need to adapt it to your specific use case. For example, you'll need to implement the code for training each branch, checking convergence on the validation set, and deciding when to switch from training one branch to the other."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
